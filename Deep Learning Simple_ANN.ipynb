{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c97adb9-b2df-4fc6-99a3-67b4cb3411d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x4 tensor with random floating-point values between 0 and 1 after setting the seed to 42. \n",
    "#Compute the sum of all elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b521d976-cc38-42f0-9a55-7745eac6267f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.3908)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "x = torch.rand(4,4)\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfc436d-d8ee-40a2-882e-34c2097438eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor x with the value 3.0 and set requires_grad=True. Define y = x^2 + 2*x. Compute the gradient of y with respect to x.\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y = x**2 + 2*x\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b310631-8685-436c-9324-e6f73fd4a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Use the following small dataset with fixed values:\n",
    "\n",
    "# features = torch.tensor([[0.5, 1.0], [1.0, 2.0], [1.5, 3.0], [2.0, 4.0]])\n",
    "# labels = torch.tensor([[0.5], [1.0], [1.5], [2.0]])\n",
    "# Build a feed-forward neural network with 2 input features and 1 output, using a single hidden layer of 3 neurons. \n",
    "# Use torch.nn.Linear layers and ReLU as the activation function.\n",
    "\n",
    "# Enter the number of parameters in the network.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "features = torch.tensor([[0.5, 1.0], [1.0, 2.0], [1.5, 3.0], [2.0, 4.0]])\n",
    "labels = torch.tensor([[0.5], [1.0], [1.5], [2.0]])\n",
    "\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleANN()\n",
    "\n",
    "num_params=0\n",
    "for i in model.parameters():\n",
    "    num_params=num_params+i.numel()\n",
    "\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "41a30c7a-2be8-4d52-a00d-7ce2ea4c09f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.483702463810914e-14"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network for 100 epochs using Mean Squared Error (MSE) loss and Stochastic Gradient Descent (SGD) optimizer. Set the learning rate to 0.01.\n",
    "\n",
    "# Enter the final loss after 100 epochs\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features)\n",
    "    loss = criterion(output,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "final_loss = loss.item()\n",
    "final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "624604f5-091a-4f5c-8f17-bebb0cb71b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9115620851516724"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the same network from Question 3, change the activation function in the hidden layer from ReLU to Sigmoid. Train for 100 epochs. How does the final loss compare to the one obtained with ReLU?\n",
    "\n",
    "# A) Loss decreases.\n",
    "# B) Loss increases.\n",
    "# C) Loss stays the same.\n",
    "# D) Cannot be determined.\n",
    "\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,3)\n",
    "        self.relu = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleANN()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features)\n",
    "    loss = criterion(output,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "final_loss = loss.item()\n",
    "final_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
